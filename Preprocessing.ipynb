{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function reads the csv file and keeps the useful columns\n",
    "def load_raw_data(filename):\n",
    "    '''This function reads a raw csv file and keeps all the useful columns \n",
    "    INPUT: the name of the file\n",
    "    OUTPUT: a matrix of raw features and a vector with the postal codes'''\n",
    "    X = []\n",
    "    y = []\n",
    "    with open(filename, newline='',encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        count = 0\n",
    "        fields = 0\n",
    "        for row in reader:\n",
    "            if (count == 0):\n",
    "                count += 1\n",
    "                fields = len(row)\n",
    "            # We keep only the questions with a valid ID and a valid number of rows\n",
    "            if ((len(row[0])==60) and len(row)==fields) :\n",
    "                data_vector = np.array(row)\n",
    "                dvx = data_vector[[2,9]]\n",
    "                dvx = np.append(dvx,data_vector[11:])\n",
    "                X.append(dvx)\n",
    "                dvy = data_vector[10]\n",
    "                y.append(dvy)\n",
    "                # We do this in order to free up memory\n",
    "                data_vector = None\n",
    "        #  call the garbage collector\n",
    "        gc.collect()\n",
    "    return X, y\n",
    "\n",
    "# This function creates a column vector from a matrix\n",
    "def column(matrix, i):\n",
    "    return [row[i] for row in matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charging the data from the csv file\n",
    "X,y = load_raw_data(\"LA_TRANSITION_ECOLOGIQUE.csv\")\n",
    "\n",
    "# Preprocessing of the ZIP code. We just need to convert it from str to int\n",
    "y = np.array(y)\n",
    "y = np.asarray(y,dtype='int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/QQINO/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/QQINO/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk #import the natural language toolkit library\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.snowball import FrenchStemmer #import the French stemming library\n",
    "from nltk.corpus import stopwords #import stopwords from nltk corpus\n",
    "from collections import Counter #allows for counting the number of occurences in a list\n",
    "\n",
    "def get_tokens(raw,encoding='utf8'):\n",
    "    '''get the nltk tokens from a text'''\n",
    "    tokens = nltk.word_tokenize(raw) #tokenize the raw UTF-8 text\n",
    "    return tokens\n",
    "\n",
    "def get_nltk_text(raw,encoding='utf8'):\n",
    "    '''create an nltk text using the passed argument (raw) after filtering out the commas'''\n",
    "    #turn the raw text into an nltk text object\n",
    "    no_commas = re.sub(r'[.|,|\\'|;|\\\"|(|)|’]',' ', raw) #filter out all the commas, periods, and appostrophes using regex\n",
    "    tokens = nltk.word_tokenize(no_commas) #generate a list of tokens from the raw text\n",
    "    text=nltk.Text(tokens,encoding) #create a nltk text from those tokens\n",
    "    return text\n",
    "\n",
    "def no_accents(raw,encoding='utf8'):\n",
    "    '''turns any special character to standard ones.'''\n",
    "    no_accent_a = re.sub(r'[à|â]','a', raw)\n",
    "    no_accent_c = re.sub(r'[ç]','c', no_accent_a)\n",
    "    no_accent_e = re.sub(r'[è|é|ê|ë]','e', no_accent_c)\n",
    "    no_accent_i = re.sub(r'[î|ï]','i', no_accent_e)\n",
    "    no_accent_o = re.sub(r'[ô]','o', no_accent_i)\n",
    "    no_accent_u = re.sub(r'[û|ù|ü]','u', no_accent_o)\n",
    "    \n",
    "    return no_accent_u\n",
    "\n",
    "def get_stopswords(type=\"veronis\"):\n",
    "    '''returns the veronis stopwords in unicode, or if any other value is passed, it returns the default nltk french stopwords'''\n",
    "    if type==\"veronis\":\n",
    "        #VERONIS STOPWORDS\n",
    "        raw_stopword_list = [\"Ap.\", \"Apr.\", \"GHz\", \"MHz\", \"USD\", \"a\", \"afin\", \"ah\", \"ai\", \"aie\", \"aient\", \"aies\", \"ait\", \"alors\", \"après\", \"as\", \"attendu\", \"au\", \"au-delà\", \"au-devant\", \"aucun\", \"aucune\", \"audit\", \"auprès\", \"auquel\", \"aura\", \"aurai\", \"auraient\", \"aurais\", \"aurait\", \"auras\", \"aurez\", \"auriez\", \"aurions\", \"aurons\", \"auront\", \"aussi\", \"autour\", \"autre\", \"autres\", \"autrui\", \"aux\", \"auxdites\", \"auxdits\", \"auxquelles\", \"auxquels\", \"avaient\", \"avais\", \"avait\", \"avant\", \"avec\", \"avez\", \"aviez\", \"avions\", \"avons\", \"ayant\", \"ayez\", \"ayons\", \"b\", \"bah\", \"banco\", \"ben\", \"bien\", \"bé\", \"c\", \"c'\", \"c'est\", \"c'était\", \"car\", \"ce\", \"ceci\", \"cela\", \"celle\", \"celle-ci\", \"celle-là\", \"celles\", \"celles-ci\", \"celles-là\", \"celui\", \"celui-ci\", \"celui-là\", \"celà\", \"cent\", \"cents\", \"cependant\", \"certain\", \"certaine\", \"certaines\", \"certains\", \"ces\", \"cet\", \"cette\", \"ceux\", \"ceux-ci\", \"ceux-là\", \"cf.\", \"cg\", \"cgr\", \"chacun\", \"chacune\", \"chaque\", \"chez\", \"ci\", \"cinq\", \"cinquante\", \"cinquante-cinq\", \"cinquante-deux\", \"cinquante-et-un\", \"cinquante-huit\", \"cinquante-neuf\", \"cinquante-quatre\", \"cinquante-sept\", \"cinquante-six\", \"cinquante-trois\", \"cl\", \"cm\", \"cm²\", \"comme\", \"contre\", \"d\", \"d'\", \"d'après\", \"d'un\", \"d'une\", \"dans\", \"de\", \"depuis\", \"derrière\", \"des\", \"desdites\", \"desdits\", \"desquelles\", \"desquels\", \"deux\", \"devant\", \"devers\", \"dg\", \"différentes\", \"différents\", \"divers\", \"diverses\", \"dix\", \"dix-huit\", \"dix-neuf\", \"dix-sept\", \"dl\", \"dm\", \"donc\", \"dont\", \"douze\", \"du\", \"dudit\", \"duquel\", \"durant\", \"dès\", \"déjà\", \"e\", \"eh\", \"elle\", \"elles\", \"en\", \"en-dehors\", \"encore\", \"enfin\", \"entre\", \"envers\", \"es\", \"est\", \"et\", \"eu\", \"eue\", \"eues\", \"euh\", \"eurent\", \"eus\", \"eusse\", \"eussent\", \"eusses\", \"eussiez\", \"eussions\", \"eut\", \"eux\", \"eûmes\", \"eût\", \"eûtes\", \"f\", \"fait\", \"fi\", \"flac\", \"fors\", \"furent\", \"fus\", \"fusse\", \"fussent\", \"fusses\", \"fussiez\", \"fussions\", \"fut\", \"fûmes\", \"fût\", \"fûtes\", \"g\", \"gr\", \"h\", \"ha\", \"han\", \"hein\", \"hem\", \"heu\", \"hg\", \"hl\", \"hm\", \"hm³\", \"holà\", \"hop\", \"hormis\", \"hors\", \"huit\", \"hum\", \"hé\", \"i\", \"ici\", \"il\", \"ils\", \"j\", \"j'\", \"j'ai\", \"j'avais\", \"j'étais\", \"jamais\", \"je\", \"jusqu'\", \"jusqu'au\", \"jusqu'aux\", \"jusqu'à\", \"jusque\", \"k\", \"kg\", \"km\", \"km²\", \"l\", \"l'\", \"l'autre\", \"l'on\", \"l'un\", \"l'une\", \"la\", \"laquelle\", \"le\", \"lequel\", \"les\", \"lesquelles\", \"lesquels\", \"leur\", \"leurs\", \"lez\", \"lors\", \"lorsqu'\", \"lorsque\", \"lui\", \"lès\", \"m\", \"m'\", \"ma\", \"maint\", \"mainte\", \"maintes\", \"maints\", \"mais\", \"malgré\", \"me\", \"mes\", \"mg\", \"mgr\", \"mil\", \"mille\", \"milliards\", \"millions\", \"ml\", \"mm\", \"mm²\", \"moi\", \"moins\", \"mon\", \"moyennant\", \"mt\", \"m²\", \"m³\", \"même\", \"mêmes\", \"n\", \"n'avait\", \"n'y\", \"ne\", \"neuf\", \"ni\", \"non\", \"nonante\", \"nonobstant\", \"nos\", \"notre\", \"nous\", \"nul\", \"nulle\", \"nº\", \"néanmoins\", \"o\", \"octante\", \"oh\", \"on\", \"ont\", \"onze\", \"or\", \"ou\", \"outre\", \"où\", \"p\", \"par\", \"par-delà\", \"parbleu\", \"parce\", \"parmi\", \"pas\", \"passé\", \"pendant\", \"personne\", \"peu\", \"plus\", \"plus_d'un\", \"plus_d'une\", \"plusieurs\", \"pour\", \"pourquoi\", \"pourtant\", \"pourvu\", \"près\", \"puisqu'\", \"puisque\", \"q\", \"qu\", \"qu'\", \"qu'elle\", \"qu'elles\", \"qu'il\", \"qu'ils\", \"qu'on\", \"quand\", \"quant\", \"quarante\", \"quarante-cinq\", \"quarante-deux\", \"quarante-et-un\", \"quarante-huit\", \"quarante-neuf\", \"quarante-quatre\", \"quarante-sept\", \"quarante-six\", \"quarante-trois\", \"quatorze\", \"quatre\", \"quatre-vingt\", \"quatre-vingt-cinq\", \"quatre-vingt-deux\", \"quatre-vingt-dix\", \"quatre-vingt-dix-huit\", \"quatre-vingt-dix-neuf\", \"quatre-vingt-dix-sept\", \"quatre-vingt-douze\", \"quatre-vingt-huit\", \"quatre-vingt-neuf\", \"quatre-vingt-onze\", \"quatre-vingt-quatorze\", \"quatre-vingt-quatre\", \"quatre-vingt-quinze\", \"quatre-vingt-seize\", \"quatre-vingt-sept\", \"quatre-vingt-six\", \"quatre-vingt-treize\", \"quatre-vingt-trois\", \"quatre-vingt-un\", \"quatre-vingt-une\", \"quatre-vingts\", \"que\", \"quel\", \"quelle\", \"quelles\", \"quelqu'\", \"quelqu'un\", \"quelqu'une\", \"quelque\", \"quelques\", \"quelques-unes\", \"quelques-uns\", \"quels\", \"qui\", \"quiconque\", \"quinze\", \"quoi\", \"quoiqu'\", \"quoique\", \"r\", \"revoici\", \"revoilà\", \"rien\", \"s\", \"s'\", \"sa\", \"sans\", \"sauf\", \"se\", \"seize\", \"selon\", \"sept\", \"septante\", \"sera\", \"serai\", \"seraient\", \"serais\", \"serait\", \"seras\", \"serez\", \"seriez\", \"serions\", \"serons\", \"seront\", \"ses\", \"si\", \"sinon\", \"six\", \"soi\", \"soient\", \"sois\", \"soit\", \"soixante\", \"soixante-cinq\", \"soixante-deux\", \"soixante-dix\", \"soixante-dix-huit\", \"soixante-dix-neuf\", \"soixante-dix-sept\", \"soixante-douze\", \"soixante-et-onze\", \"soixante-et-un\", \"soixante-et-une\", \"soixante-huit\", \"soixante-neuf\", \"soixante-quatorze\", \"soixante-quatre\", \"soixante-quinze\", \"soixante-seize\", \"soixante-sept\", \"soixante-six\", \"soixante-treize\", \"soixante-trois\", \"sommes\", \"son\", \"sont\", \"sous\", \"soyez\", \"soyons\", \"suis\", \"suite\", \"sur\", \"sus\", \"t\", \"t'\", \"ta\", \"tacatac\", \"tandis\", \"te\", \"tel\", \"telle\", \"telles\", \"tels\", \"tes\", \"toi\", \"ton\", \"toujours\", \"tous\", \"tout\", \"toute\", \"toutefois\", \"toutes\", \"treize\", \"trente\", \"trente-cinq\", \"trente-deux\", \"trente-et-un\", \"trente-huit\", \"trente-neuf\", \"trente-quatre\", \"trente-sept\", \"trente-six\", \"trente-trois\", \"trois\", \"très\", \"tu\", \"u\", \"un\", \"une\", \"unes\", \"uns\", \"v\", \"vs\", \"vers\", \"via\", \"vingt\", \"vingt-cinq\", \"vingt-deux\", \"vingt-huit\", \"vingt-neuf\", \"vingt-quatre\", \"vingt-sept\", \"vingt-six\", \"vingt-trois\", \"vis-à-vis\", \"voici\", \"voilà\", \"vos\", \"votre\", \"vous\", \"w\", \"x\", \"y\", \"z\", \"zéro\", \"à\", \"ç'\", \"ça\", \"ès\", \"étaient\", \"étais\", \"était\", \"étant\", \"étiez\", \"étions\", \"été\", \"étée\", \"étées\", \"étés\", \"êtes\", \"être\", \"ô\"]\n",
    "    else:\n",
    "        #get French stopwords from the nltk kit\n",
    "        raw_stopword_list = stopwords.words('french') #create a list of all French stopwords\n",
    "    stopword_list = raw_stopword_list\n",
    "    #stopword_list = [word.decode('utf8') for word in raw_stopword_list] #make to decode the French stopwords as unicode objects rather than ascii\n",
    "    return stopword_list\n",
    "\n",
    "def filter_stopwords(text,stopword_list):\n",
    "    '''normalizes the words by turning them all lowercase and then filters out the stopwords'''\n",
    "    lower_words=[w.lower() for w in text] #normalize the words in the text, making them all lowercase\n",
    "    words=[no_accents(w) for w in lower_words]\n",
    "    #filtering stopwords\n",
    "    filtered_words = [] #declare an empty list to hold our filtered words\n",
    "    for word in words: #iterate over all words from the text\n",
    "        if word not in stopword_list and word.isalpha() and len(word) > 1: #only add words that are not in the French stopwords list, are alphabetic, and are more than 1 character\n",
    "            filtered_words.append(word) #add word to filter_words list if it meets the above conditions\n",
    "    filtered_words.sort() #sort filtered_words list\n",
    "    return filtered_words\n",
    "\n",
    "def stem_words(words):\n",
    "    '''stems the word list using the French Stemmer'''\n",
    "    #stemming words\n",
    "    stemmed_words = [] #declare an empty list to hold our stemmed words\n",
    "    stemmer = FrenchStemmer() #create a stemmer object in the FrenchStemmer class\n",
    "    for word in words:\n",
    "        stemmed_word=stemmer.stem(word) #stem the word\n",
    "        stemmed_words.append(stemmed_word) #add it to our stemmed word list\n",
    "    stemmed_words.sort() #sort the stemmed_words\n",
    "    return stemmed_words\n",
    "\n",
    "def sort_dictionary(dictionary):\n",
    "    '''returns a sorted dictionary (as tuples) based on the value of each key'''\n",
    "    return sorted(dictionary.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def normalize_counts(counts):\n",
    "    '''returns the frequency of tokens for each text'''\n",
    "    total = sum(counts.values())\n",
    "    return dict((word, float(count)/total) for word,count in counts.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(word_vector):\n",
    "    '''This function takes a vector of questions and filters out all the spaces, the suffixes\n",
    "    and prefixes, the special charaters and counts the number of occurrences.\n",
    "    INPUT: a vector of texts\n",
    "    OUTPUT a vector of dictionaries which represents a text'''\n",
    "    vector = []\n",
    "    for i in word_vector:\n",
    "        # TODO: Remove tabs (\\t) too\n",
    "        raw = re.sub(r'\\s+', ' ',i) #remove the excess whitespace from the raw text\n",
    "        text = get_nltk_text(raw)\n",
    "        stopword_list = get_stopswords()\n",
    "        filtered_text = filter_stopwords(text,stopword_list)\n",
    "        stemed_text = stem_words(filtered_text)\n",
    "        bag_words = sort_dictionary(Counter(stemed_text))\n",
    "        vector.append(bag_words)\n",
    "    gc.collect()\n",
    "    return vector\n",
    "\n",
    "def preprocessing_yesno(word_vector):\n",
    "    '''This function takes a vector of yes and no and make them into an int (yes 1)(no -1)(unanswered 0)\n",
    "    INPUT: a vector of yes and no answers\n",
    "    OUTPUT a vector of int according to the classification'''\n",
    "    vector = []\n",
    "    for i in word_vector:\n",
    "        if i == 'Oui':\n",
    "            vector.append(1)\n",
    "        elif i == 'Non':\n",
    "            vector.append(-1)\n",
    "        else: vector.append(0)\n",
    "    return vector\n",
    "\n",
    "def preprocessing_categories(word_vector,category_vector):\n",
    "    '''This function labels the an array of words which represents categories\n",
    "    INPUT: a vector of answers, a vector of categories to label the answers\n",
    "    OUTPUT a vector with the label for each category\n",
    "    Nb: 0 if No category'''\n",
    "    vector = []\n",
    "    for i in word_vector:\n",
    "        found = False\n",
    "        for j in range(len(category_vector)):\n",
    "            if i == category_vector[j]:\n",
    "                found = True\n",
    "                vector.append(j+1)\n",
    "                break\n",
    "        if not found : vector.append(0)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_raw_data(X,y,array_yes_no,array_category,category_labels):\n",
    "    '''This function process all the columns of features and fussion them in\n",
    "    a unique array with the zip code\n",
    "    INPUT: raw features, zip code, column number of yes/no questions and text labels\n",
    "    for category question\n",
    "    OUTPUT: the processed data whose columns are in this order:\n",
    "        Title, Category, Questions 1 - 14, Zip code'''\n",
    "    # Preprocessing questions\n",
    "    questions = []\n",
    "    n = len(X[0])\n",
    "    for i in range(n):\n",
    "        print(\"There are\",len(X[0]),\" iterations. Count: \",i)\n",
    "        if i in array_yes_no:\n",
    "            questions.append(preprocessing_yesno(column(X,i)))\n",
    "        elif i in array_category:\n",
    "            questions.append(preprocessing_categories(column(X,i),category_labels.pop(0)))\n",
    "        else:\n",
    "            questions.append(preprocessing_text(column(X,i)))\n",
    "\n",
    "    questions_trans = np.transpose(np.array(questions))\n",
    "    output = np.array(y).reshape((len(y),1))\n",
    "    data = np.concatenate((questions_trans,output),axis=1)\n",
    "    gc.collect()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18  iterations. Count:  0\n",
      "There are 18  iterations. Count:  1\n",
      "There are 18  iterations. Count:  2\n",
      "There are 18  iterations. Count:  3\n",
      "There are 18  iterations. Count:  4\n",
      "There are 18  iterations. Count:  5\n",
      "There are 18  iterations. Count:  6\n",
      "There are 18  iterations. Count:  7\n",
      "There are 18  iterations. Count:  8\n",
      "There are 18  iterations. Count:  9\n",
      "There are 18  iterations. Count:  10\n",
      "There are 18  iterations. Count:  11\n",
      "There are 18  iterations. Count:  12\n",
      "There are 18  iterations. Count:  13\n",
      "There are 18  iterations. Count:  14\n",
      "There are 18  iterations. Count:  15\n",
      "There are 18  iterations. Count:  16\n",
      "There are 18  iterations. Count:  17\n"
     ]
    }
   ],
   "source": [
    "# Treatement of Transition_ecologique dataset\n",
    "user_category = ['Citoyen / Citoyenne','Élu / élue et Institution',\n",
    "                 'Organisation à but lucratif','Organisation à but non lucratif']\n",
    "\n",
    "mobilite_category = ['Oui','Non',\"Je n'utilise pas la voiture pour des déplacements quotidiens\"]\n",
    "labels = [user_category,mobilite_category]\n",
    "data = preprocessing_raw_data(X,y,[4,6,10],[1,12],labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(word_vector,filename):\n",
    "    '''return a csv file from a vector which contains int and arrays of dictionaries\n",
    "    INPUT: matrix to be written, name of the file'''\n",
    "    with open(filename, 'w', newline='',encoding='utf-8') as f:\n",
    "        writer = csv.writer(f,delimiter=';',quoting=csv.QUOTE_NONE)\n",
    "        writer.writerows(word_vector)\n",
    "\n",
    "def read_data(filename,yes_no_array):\n",
    "    '''return an array of cleaned data from a csv file made by the write_data function.\n",
    "    The order of the columns is the following: \n",
    "    Title | Category of the participant | Questions | Output\n",
    "    INPUT: name of the file, position of the integer columns\n",
    "    OUTPUT: matrix of data'''\n",
    "    data = []\n",
    "    with open(filename, newline='',encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=';')\n",
    "        for row in reader:\n",
    "            element = []\n",
    "            for i in range(len(row)-1):\n",
    "                if i in yes_no_array:\n",
    "                    element.append(int(row[i]))\n",
    "                else: element.append(read_dictionary(row[i]))\n",
    "            element.append(row.pop())\n",
    "            data.append(element)\n",
    "        #  call the garbage collector\n",
    "        gc.collect()\n",
    "    return data\n",
    "\n",
    "def read_word_count(filename):\n",
    "    '''This function reads a file which contains arrays of tuples of key and number of occurrences\n",
    "    of a word, and returns an array of dictionaries with the tuples\n",
    "    INPUT: name of the file to upload\n",
    "    OUTPUT: dictionnary with the words of the file'''\n",
    "    words = []\n",
    "    with open(filename, newline='',encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=';')\n",
    "        for row in reader:\n",
    "            dictionary = dict()\n",
    "            for i in row:\n",
    "                dictionary.update(read_dictionary(i))\n",
    "            words.append(dictionary)\n",
    "        #  call the garbage collector\n",
    "        gc.collect()\n",
    "    return words\n",
    "\n",
    "def read_dictionary(dictionary_string):\n",
    "    '''return a dictionary from a string \n",
    "    INPUT: an string of many key-value tuples\n",
    "    OUTPUT: a dictionary'''\n",
    "    dictionary = dict()\n",
    "    clean_dict = re.sub(r'[ \\[ | \\] | \\( | \\) | \\' | { | } ]','', dictionary_string)\n",
    "    parsed_dict = re.split(r'[,|:]', clean_dict)\n",
    "    for i in range(int(len(parsed_dict)/2)):\n",
    "        key = parsed_dict[2*i]\n",
    "        value = int(parsed_dict[2*i+1])\n",
    "        dictionary[key] = value\n",
    "    return dictionary\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_data(data,'data.csv')\n",
    "\n",
    "data = read_data('data.csv',[1,4,6,10,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entries_pattern(data,pattern):\n",
    "    ''' return an array with all the entries of a dataset whose postal code begins\n",
    "    with a given pattern\n",
    "    INPUT: matrix, pattern to be searched\n",
    "    OUTPUT: an array with all the entries which match the pattern'''\n",
    "    entries = []\n",
    "    for entry in data:\n",
    "        match = re.match(pattern,entry[-1])\n",
    "        if (match != None):\n",
    "            entries.append(entry)\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = find_entries_pattern(data,'511')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_zip_codes_by_village(density_threshold,filename='correspondance-code-insee-code-postal.csv'):\n",
    "    '''This function splits up the zip codes of a file into two categories according to a density threshold\n",
    "    INPUT: density threshold, file name of the reference library\n",
    "    OUTPUT: set of zip codes for cities, a set of zip codes for villages'''\n",
    "    codes_cities = set()\n",
    "    tmp_code_cities = set()\n",
    "    codes_villages = set()\n",
    "    tmp_code_villages = set()\n",
    "    with open(filename, newline='',encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=';')\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            density = -1\n",
    "            # We keep only the questions with a valid ID and a valid number of rows\n",
    "            if row[8] != '' and row[7] != '':\n",
    "                density = float(row[8])*1000 / float(row[7])\n",
    "            if density >= density_threshold:\n",
    "                codes_cities.add(row[1])\n",
    "            elif density != -1: codes_villages.add(row[1])\n",
    "    tmp_code_cities = codes_cities.copy()\n",
    "    tmp_code_villages = codes_villages.copy()\n",
    "    for code in tmp_code_cities:\n",
    "        match = re.search('/',code)\n",
    "        if (match != None):\n",
    "            codes_cities.remove(code)\n",
    "            split_codes = re.split('/', code)\n",
    "            for i in split_codes:\n",
    "                codes_cities.add(i)\n",
    "    for code in tmp_code_villages:\n",
    "        match = re.search('/',code)\n",
    "        if (match != None):\n",
    "            codes_villages.remove(code)\n",
    "            split_codes = re.split('/', code)\n",
    "            for i in split_codes:\n",
    "                codes_villages.add(i)\n",
    "    #  call the garbage collector\n",
    "    gc.collect()\n",
    "    return codes_cities,codes_villages\n",
    "\n",
    "\n",
    "def find_zip_codes_by_town(density_threshold,filename='city_information.tsv'):\n",
    "    '''This function splits up the zip codes of a file into two categories according to a density threshold\n",
    "    INPUT: density threshold, file name of the reference library\n",
    "    OUTPUT: set of zip codes for cities, a set of zip codes for villages'''\n",
    "    codes_cities = set()\n",
    "    tmp_code_cities = set()\n",
    "    codes_villages = set()\n",
    "    tmp_code_villages = set()\n",
    "    with open(filename, newline='',encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            # We keep only the questions with a valid ID and a valid number of rows\n",
    "            density = -1\n",
    "            if row[3] != '' and row[4] != '':\n",
    "                density = float(row[3]) / float(row[4])\n",
    "            if density >= density_threshold:\n",
    "                codes_cities.add(row[1])\n",
    "            elif density != -1: codes_villages.add(row[1])\n",
    "    tmp_code_cities = codes_cities.copy()\n",
    "    tmp_code_villages = codes_villages.copy()\n",
    "    for code in tmp_code_cities:\n",
    "        match = re.search('-',code)\n",
    "        if (match != None):\n",
    "            codes_cities.remove(code)\n",
    "            split_codes = re.split('-', code)\n",
    "            for i in split_codes:\n",
    "                codes_cities.add(i)\n",
    "    for code in tmp_code_villages:\n",
    "        match = re.search('-',code)\n",
    "        if (match != None):\n",
    "            codes_villages.remove(code)\n",
    "            split_codes = re.split('-', code)\n",
    "            for i in split_codes:\n",
    "                codes_villages.add(i)\n",
    "    #  call the garbage collector\n",
    "    gc.collect()\n",
    "    return codes_cities,codes_villages\n",
    "\n",
    "def city_village_classifier(density_threshold,data):\n",
    "    '''This function tags the entries of a data set according to their population density. If it is more than\n",
    "    a given threshold, the label is 1, -1 otherwise\n",
    "    INPUT: the density threshold, a dataset\n",
    "    OUTPUT: the classified dataset'''\n",
    "    city_zip_codes , village_zip_codes = find_zip_codes_by_town(density_threshold)\n",
    "    classified_data = []\n",
    "    class_vector = []\n",
    "    for entry in data:\n",
    "        if entry[-1] in city_zip_codes:\n",
    "            classified_data.append(entry)\n",
    "            class_vector.append(1)\n",
    "        elif entry[-1] in village_zip_codes: \n",
    "            classified_data.append(entry)\n",
    "            class_vector.append(-1)\n",
    "    classified_data = np.array(classified_data)\n",
    "    classified_data = np.delete(classified_data,len(classified_data[0])-1,1)\n",
    "    class_vector = np.array(class_vector).reshape((len(class_vector),1))\n",
    "    classified_data = np.append(classified_data,class_vector,axis=1)\n",
    "    #  call the garbage collector\n",
    "    gc.collect()\n",
    "    return classified_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_data = city_village_classifier(20344,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ecolog': 1, 'transit': 1} 1 {} {} 0 {} 0 {} {} {} 0 {} 0 {} {} {}\n",
      " {'ecol': 1, 'enseign': 1, 'select': 1, 'tri': 1}\n",
      " {'central': 1, 'geotherm': 1, 'multipli': 1} -1]\n"
     ]
    }
   ],
   "source": [
    "print(classified_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_by_question(data,yes_no_questions):\n",
    "    '''This function counts all the words of a data set by question(column) and arrange them into a dictionnary\n",
    "    INPUT:  dataset, the numbers columns of integers\n",
    "    OUTPUT: an array of dictionaries of the counted words\n",
    "    '''\n",
    "    word_count = []    \n",
    "    for i in range(len(data[0])-1):\n",
    "        if i not in yes_no_questions:\n",
    "            word_count.append(Counter())\n",
    "    j = 0\n",
    "    for i in range(len(data[0])-1):\n",
    "        if i not in yes_no_questions:\n",
    "            for entry in column(data,i): word_count[j] += Counter(entry)\n",
    "            j += 1\n",
    "    #  call the garbage collector\n",
    "    gc.collect()\n",
    "    return word_count\n",
    "\n",
    "def word_count_total(dictionary_array):\n",
    "    '''This functions sums up all the words of a many texts represented by dictionaries\n",
    "    INPUT: an array of dictionaries\n",
    "    OUTPUT a dictionary with the count of all the words:\n",
    "    '''\n",
    "    total_words = Counter()\n",
    "    for question in dictionary_array: total_words += Counter(question)\n",
    "    return total_words\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = word_count_by_question(classified_data,[1,4,6,10,12])\n",
    "f = word_count_total(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data([sort_dictionary(i) for i in e],'word_count_by_question.csv')\n",
    "write_data(sort_dictionary(f),'word_count_total.csv')\n",
    "\n",
    "g = read_word_count('word_count_by_question.csv')\n",
    "h = read_word_count('word_count_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_used_words(data, yes_no_array, word_count_array, number_of_words):\n",
    "    '''This function compares for each entry and question the number of ocurrences of the most used\n",
    "    words in the respective question. This function limits the number of most used words in order to normalized\n",
    "    the output\n",
    "    INPUT: a data set, the answers whose response is an integer, an array with the total of words by questions\n",
    "    the maximum number of words to keep, \n",
    "    OUTPUT: the filtered dataset\n",
    "    '''\n",
    "    most_used_words = []\n",
    "    for question in word_count_array:\n",
    "        words_by_question = []\n",
    "        sorted_word_array = sort_dictionary(question)\n",
    "        for i in range(number_of_words):\n",
    "            words_by_question.append(sorted_word_array[i])\n",
    "        most_used_words.append(words_by_question)\n",
    "    \n",
    "    filtered_data = []\n",
    "    for entry in data:\n",
    "        entry_array = []\n",
    "        j = 0\n",
    "        for i in range(len(entry)-1):\n",
    "            if i in yes_no_array:\n",
    "                entry_array.append(entry[i])\n",
    "            else:\n",
    "                words_rep_array = []\n",
    "                for word in most_used_words[j]:\n",
    "                    try:\n",
    "                        words_rep_array.append(entry[i][word[0]])\n",
    "                    except KeyError as error:\n",
    "                        words_rep_array.append(0)\n",
    "                entry_array.append(words_rep_array)\n",
    "                j += 1\n",
    "        entry_array.append(entry[-1])\n",
    "        filtered_data.append(entry_array)\n",
    "        \n",
    "    #  call the garbage collector\n",
    "    gc.collect()\n",
    "        \n",
    "    return filtered_data\n",
    "\n",
    "def get_set_features(data,column):\n",
    "    '''This function return a feature (column) of a dataset\n",
    "    '''\n",
    "    a_data = np.array(data)\n",
    "    features = a_data[:,column]\n",
    "    features = np.array([np.asarray(i) for i in features])\n",
    "    return features\n",
    "\n",
    "def get_total_most_used_words(data, yes_no_array, word_count_array, number_of_words,by_question=True):\n",
    "    '''This function compares for each entry and question the number of ocurrences of the most used\n",
    "    words in the respective question. This function limits the number of most used words in order to normalized\n",
    "    the output\n",
    "    INPUT: a data set, the answers whose response is an integer, an array with the total number of words\n",
    "    the maximum number of words to keep, \n",
    "    OUTPUT: the filtered dataset\n",
    "    '''\n",
    "    most_used_words = []\n",
    "    sorted_word_array = sort_dictionary(word_count_array)\n",
    "    for i in range(number_of_words):\n",
    "        most_used_words.append(sorted_word_array[i])\n",
    "    \n",
    "    filtered_data = []\n",
    "    for entry in data:\n",
    "        entry_array = []\n",
    "        j = 0\n",
    "        entry_words = word_count_by_question([entry],yes_no_array)\n",
    "        entry_words= word_count_total(entry_words)\n",
    "        words_rep_array = []\n",
    "        for word in most_used_words:\n",
    "            try:\n",
    "                words_rep_array.append(entry_words[word[0]])\n",
    "            except KeyError as error:\n",
    "                words_rep_array.append(0)\n",
    "        entry_array.append(words_rep_array)\n",
    "        j += 1\n",
    "        entry_array.append(entry[-1])\n",
    "        filtered_data.append(entry_array)\n",
    "\n",
    "    #  call the garbage collector\n",
    "    gc.collect()\n",
    "        \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = get_most_used_words(classified_data,[1,4,6,10,12],e,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 3],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_set_features(filtered_data,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-328-49b5a29060c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtotal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_total_most_used_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassified_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-327-c5008affa1be>\u001b[0m in \u001b[0;36mget_total_most_used_words\u001b[0;34m(data, yes_no_array, word_count_array, number_of_words, by_question)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mentry_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mentry_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_count_by_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myes_no_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mentry_words\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mword_count_total\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mwords_rep_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-251-0091fd35f7bd>\u001b[0m in \u001b[0;36mword_count_by_question\u001b[0;34m(data, yes_no_questions)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mj\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#  call the garbage collector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_data = get_total_most_used_words(classified_data,[1,4,6,10,12],f,20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
