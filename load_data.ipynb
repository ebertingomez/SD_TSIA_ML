{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import gc\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename,yes_no_array):\n",
    "    '''return an array of cleaned data from a csv file made by the write_data function.\n",
    "    The order of the columns is the following: \n",
    "    Title | Category of the participant | Questions | Output\n",
    "    INPUT: name of the file, position of the integer columns\n",
    "    OUTPUT: matrix of data'''\n",
    "    data = []\n",
    "    with open(filename, newline='',encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=';')\n",
    "        for row in reader:\n",
    "            element = []\n",
    "            for i in range(len(row)-1):\n",
    "                if i in yes_no_array:\n",
    "                    element.append(int(row[i]))\n",
    "                else: element.append(read_dictionary(row[i]))\n",
    "            element.append(row.pop())\n",
    "            data.append(element)\n",
    "        #  call the garbage collector\n",
    "        gc.collect()\n",
    "    return data\n",
    "\n",
    "def read_word_count(filename):\n",
    "    '''This function reads a file which contains arrays of tuples of key and number of occurrences\n",
    "    of a word, and returns an array of dictionaries with the tuples\n",
    "    INPUT: name of the file to upload\n",
    "    OUTPUT: dictionnary with the words of the file'''\n",
    "    words = []\n",
    "    with open(filename, newline='',encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=';')\n",
    "        for row in reader:\n",
    "            dictionary = dict()\n",
    "            for i in row:\n",
    "                dictionary.update(read_dictionary(i))\n",
    "            words.append(dictionary)\n",
    "        #  call the garbage collector\n",
    "        gc.collect()\n",
    "    return words\n",
    "\n",
    "def read_dictionary(dictionary_string):\n",
    "    '''return a dictionary from a string \n",
    "    INPUT: an string of many key-value tuples\n",
    "    OUTPUT: a dictionary'''\n",
    "    dictionary = dict()\n",
    "    clean_dict = re.sub(r'[ \\[ | \\] | \\( | \\) | \\' | { | } ]','', dictionary_string)\n",
    "    parsed_dict = re.split(r'[,|:]', clean_dict)\n",
    "    for i in range(int(len(parsed_dict)/2)):\n",
    "        key = parsed_dict[2*i]\n",
    "        value = int(parsed_dict[2*i+1])\n",
    "        dictionary[key] = value\n",
    "    return dictionary\n",
    "\n",
    "def sort_dictionary(dictionary):\n",
    "    '''returns a sorted dictionary (as tuples) based on the value of each key'''\n",
    "    return sorted(dictionary.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def normalize_counts(counts):\n",
    "    '''returns the frequency of tokens for each text'''\n",
    "    total = sum(counts.values())\n",
    "    return dict((word, float(count)/total) for word,count in counts.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_zip_codes_by_town(density_threshold,filename='city_information.tsv'):\n",
    "    '''This function splits up the zip codes of a file into two categories according to a density threshold\n",
    "    INPUT: density threshold, file name of the reference library\n",
    "    OUTPUT: set of zip codes for cities, a set of zip codes for villages'''\n",
    "    codes_cities = set()\n",
    "    tmp_code_cities = set()\n",
    "    codes_villages = set()\n",
    "    tmp_code_villages = set()\n",
    "    with open(filename, newline='',encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            # We keep only the questions with a valid ID and a valid number of rows\n",
    "            density = -1\n",
    "            if row[3] != '' and row[4] != '':\n",
    "                density = float(row[3]) / float(row[4])\n",
    "            if density >= density_threshold:\n",
    "                codes_cities.add(row[1])\n",
    "            elif density != -1: codes_villages.add(row[1])\n",
    "    tmp_code_cities = codes_cities.copy()\n",
    "    tmp_code_villages = codes_villages.copy()\n",
    "    for code in tmp_code_cities:\n",
    "        match = re.search('-',code)\n",
    "        if (match != None):\n",
    "            codes_cities.remove(code)\n",
    "            split_codes = re.split('-', code)\n",
    "            for i in split_codes:\n",
    "                codes_cities.add(i)\n",
    "    for code in tmp_code_villages:\n",
    "        match = re.search('-',code)\n",
    "        if (match != None):\n",
    "            codes_villages.remove(code)\n",
    "            split_codes = re.split('-', code)\n",
    "            for i in split_codes:\n",
    "                codes_villages.add(i)\n",
    "    #  call the garbage collector\n",
    "    gc.collect()\n",
    "    return codes_cities,codes_villages\n",
    "\n",
    "def city_village_classifier(density_threshold,data):\n",
    "    '''This function tags the entries of a data set according to their population density. If it is more than\n",
    "    a given threshold, the label is 1, -1 otherwise\n",
    "    INPUT: the density threshold, a dataset\n",
    "    OUTPUT: the classified dataset'''\n",
    "    city_zip_codes , village_zip_codes = find_zip_codes_by_town(density_threshold)\n",
    "    classified_data = []\n",
    "    class_vector = []\n",
    "    for entry in data:\n",
    "        if entry[-1] in city_zip_codes:\n",
    "            classified_data.append(entry)\n",
    "            class_vector.append(1)\n",
    "        elif entry[-1] in village_zip_codes: \n",
    "            classified_data.append(entry)\n",
    "            class_vector.append(-1)\n",
    "    classified_data = np.array(classified_data)\n",
    "    classified_data = np.delete(classified_data,len(classified_data[0])-1,1)\n",
    "    class_vector = np.array(class_vector).reshape((len(class_vector),1))\n",
    "    classified_data = np.append(classified_data,class_vector,axis=1)\n",
    "    #  call the garbage collector\n",
    "    gc.collect()\n",
    "    return classified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_by_question(data,yes_no_questions):\n",
    "    '''This function counts all the words of a data set by question(column) and arrange them into a dictionnary\n",
    "    INPUT:  dataset, the numbers columns of integers\n",
    "    OUTPUT: an array of dictionaries of the counted words\n",
    "    '''\n",
    "    word_count = []    \n",
    "    for i in range(len(data[0])-1):\n",
    "        if i not in yes_no_questions:\n",
    "            word_count.append(Counter())\n",
    "    j = 0\n",
    "    for i in range(len(data[0])-1):\n",
    "        if i not in yes_no_questions:\n",
    "            for entry in column(data,i): word_count[j] += Counter(entry)\n",
    "            j += 1\n",
    "    #  call the garbage collector\n",
    "    gc.collect()\n",
    "    return word_count\n",
    "\n",
    "def word_count_total(dictionary_array):\n",
    "    '''This functions sums up all the words of a many texts represented by dictionaries\n",
    "    INPUT: an array of dictionaries\n",
    "    OUTPUT a dictionary with the count of all the words:\n",
    "    '''\n",
    "    total_words = Counter()\n",
    "    for question in dictionary_array: total_words += Counter(question)\n",
    "    return total_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_used_words(data, yes_no_array, word_count_array, number_of_words):\n",
    "    '''This function compares for each entry and question the number of ocurrences of the most used\n",
    "    words in the respective question. This function limits the number of most used words in order to normalized\n",
    "    the output\n",
    "    INPUT: a data set, the answers whose response is an integer, an array with the total of words by questions\n",
    "    the maximum number of words to keep, \n",
    "    OUTPUT: the filtered dataset\n",
    "    '''\n",
    "    most_used_words = []\n",
    "    for question in word_count_array:\n",
    "        words_by_question = []\n",
    "        sorted_word_array = sort_dictionary(question)\n",
    "        for i in range(number_of_words):\n",
    "            words_by_question.append(sorted_word_array[i])\n",
    "        most_used_words.append(words_by_question)\n",
    "    \n",
    "    filtered_data = []\n",
    "    \n",
    "    for entry in data:\n",
    "        entry_array = []\n",
    "        j = 0\n",
    "        for i in range(len(entry)-1):\n",
    "            if i in yes_no_array:\n",
    "                entry_array.append(entry[i])\n",
    "            else:\n",
    "                words_rep_array = []\n",
    "                for word in most_used_words[j]:\n",
    "                    try:\n",
    "                        words_rep_array.append(entry[i][word[0]])\n",
    "                    except KeyError as error:\n",
    "                        words_rep_array.append(0)\n",
    "                entry_array.append(words_rep_array)\n",
    "                j += 1\n",
    "        entry_array.append(entry[-1])\n",
    "        filtered_data.append(entry_array)\n",
    "    \n",
    "    #  call the garbage collector\n",
    "    gc.collect()\n",
    "        \n",
    "    return filtered_data\n",
    "\n",
    "def get_set_features(data,column):\n",
    "    '''This function return a feature (column) of a dataset\n",
    "    '''\n",
    "    a_data = np.array(data)\n",
    "    features = a_data[:,column]\n",
    "    features = np.array([np.asarray(i) for i in features])\n",
    "    return features\n",
    "\n",
    "def get_total_most_used_words(data, yes_no_array, word_count_array, number_of_words,by_question=True):\n",
    "    '''This function compares for each entry and question the number of ocurrences of the most used\n",
    "    words in the respective question. This function limits the number of most used words in order to normalized\n",
    "    the output\n",
    "    INPUT: a data set, the answers whose response is an integer, an array with the total number of words\n",
    "    the maximum number of words to keep, \n",
    "    OUTPUT: the filtered dataset\n",
    "    '''\n",
    "    most_used_words = []\n",
    "    sorted_word_array = sort_dictionary(word_count_array)\n",
    "    for i in range(number_of_words):\n",
    "        most_used_words.append(sorted_word_array[i])\n",
    "    \n",
    "    filtered_data = []\n",
    "    for entry in data:\n",
    "        entry_array = []\n",
    "        j = 0\n",
    "        entry_words = word_count_by_question([entry],yes_no_array)\n",
    "        entry_words= word_count_total(entry_words)\n",
    "        words_rep_array = []\n",
    "        for word in most_used_words:\n",
    "            try:\n",
    "                words_rep_array.append(entry_words[word[0]])\n",
    "            except KeyError as error:\n",
    "                words_rep_array.append(0)\n",
    "        entry_array.append(words_rep_array)\n",
    "        j += 1\n",
    "        entry_array.append(entry[-1])\n",
    "        filtered_data.append(entry_array)\n",
    "\n",
    "    #  call the garbage collector\n",
    "    gc.collect()\n",
    "        \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data('data.csv',[1,4,6,10,12])\n",
    "classified_data = city_village_classifier(20344,data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = read_word_count('word_count_by_question.csv')\n",
    "h = read_word_count('word_count_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = get_most_used_words(classified_data,[1,4,6,10,12],g,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [1, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_set_features(filtered_data,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
